{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU8BYlyzjjhli7rFylX2w2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/behrangEhi/ML-DL-Projects/blob/main/Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid Activation Function & Derivative"
      ],
      "metadata": {
        "id": "UoXwZKQzrRoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantage and Disadvantage of Sigmoid Activation Function\n",
        "\n",
        "#### Advantage:\n",
        "- **Smooth Gradient:** The sigmoid activation function provides a smooth gradient, which helps prevent abrupt changes or \"jumps\" in output values during training. This smoothness aids in stable and consistent learning in neural networks[1][2].\n",
        "- **Output Range:** Output values of the sigmoid function are bound between 0 and 1, making it suitable for models where predictions need to be in the range of probabilities or percentages. This property is particularly useful in binary classification tasks where the output represents the probability of a class[1][2].\n",
        "- **Non-Linearity:** Sigmoid is a non-linear activation function, allowing neural networks to learn and model complex, non-linear relationships in the data. This non-linearity is crucial for capturing intricate patterns and structures in the input data[2].\n",
        "\n",
        "#### Disadvantage:\n",
        "- **Vanishing Gradient:** One of the main drawbacks of the sigmoid function is the issue of vanishing gradients. As the output approaches the extremes (0 or 1), the gradient of the sigmoid function becomes very small, leading to the vanishing gradient problem. This can hinder the training of deep neural networks, especially in cases where gradients become close to zero, slowing down learning or causing convergence issues[1][2].\n",
        "- **Saturated Neurons:** Sigmoid neurons can saturate, meaning that for very high or very low input values, the neuron's output becomes flat, resulting in gradients close to zero. This saturation can lead to the problem of \"dead neurons,\" where neurons stop learning effectively due to minimal gradient updates[1].\n",
        "\n",
        "In summary, while the sigmoid activation function offers advantages like smooth gradients, bounded outputs, and non-linearity, it also comes with challenges such as vanishing gradients and neuron saturation. Understanding these pros and cons is crucial for selecting the appropriate activation function based on the specific requirements and characteristics of the neural network being developed.\n",
        "\n",
        "Citations:\n",
        "\n",
        "[1] https://iq.opengenus.org/sigmoid-logistic-activation/\n",
        "\n",
        "[4] https://insideaiml.com/blog/Sigmoid-Activation-Function-1031"
      ],
      "metadata": {
        "id": "SVEmNoTrtIhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stBzcqyjoaLG"
      },
      "outputs": [],
      "source": [
        "import  numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid\n",
        "def der_sigmoid(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Generative data to plot\n",
        "x_data = np.linspace(-10, 10, 100)\n",
        "y_data = sigmoid(x_data)\n",
        "dy_data = der_sigmoid(x_data)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(x_data, y_data, x_data, dy_data)\n",
        "plt.title('Sigmoid Activation Function & Derivative')\n",
        "plt.legend(['sigmoid', 'der_sigmoid'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperbolic Tangent (tanh) Activation Function & Derivative"
      ],
      "metadata": {
        "id": "fMI4MzTprWrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantage and Disadvantage of Hyperbolic Tangent (tanh) Activation Function\n",
        "\n",
        "#### Advantage:\n",
        "- **Range:** The tanh activation function maps input values to the range of $$(-1, 1)$$, providing a wider output range compared to the sigmoid function, which ranges from 0 to 1. This broader range can help in capturing a wider variety of patterns and features in the data[1].\n",
        "- **Zero-Centered:** Tanh is a zero-centered activation function, meaning that its output has a mean value of 0. This property can aid in faster convergence during training and help mitigate issues like vanishing gradients, especially in deep neural networks[1].\n",
        "- **Non-Linearity:** Like the sigmoid function, tanh is a non-linear activation function, allowing neural networks to model complex, non-linear relationships in the data. This non-linearity is essential for capturing intricate patterns and structures in the input data[1].\n",
        "\n",
        "#### Disadvantage:\n",
        "- **Vanishing Gradient:** One of the main drawbacks of the tanh activation function is the potential for the vanishing gradient problem. As the output approaches the extremes (-1 or 1), the gradient of the tanh function becomes very small, leading to challenges in learning deep neural networks and slowing down convergence[1].\n",
        "- **Saturated Neurons:** Tanh neurons can saturate, causing the neuron's output to become flat for very high or very low input values. This saturation can result in gradients close to zero, leading to the problem of \"dead neurons\" that stop learning effectively due to minimal gradient updates[1].\n",
        "\n",
        "In summary, the tanh activation function offers advantages such as a wider output range, zero-centered output, and non-linearity, but it also comes with challenges like the vanishing gradient problem and neuron saturation. Understanding these pros and cons is crucial for selecting the appropriate activation function based on the specific requirements and characteristics of the neural network being developed.\n",
        "\n",
        "Citations:\n",
        "\n",
        "[1] https://www.aitude.com/comparison-of-sigmoid-tanh-and-relu-activation-functions/"
      ],
      "metadata": {
        "id": "wNocTQips8u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperbolic Tangent (htan) Activation Function\n",
        "def htan(x):\n",
        " return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "# htan derivative\n",
        "def der_htan(x):\n",
        " return 1 - htan(x) * htan(x)\n",
        "# Generating data for Graph\n",
        "x_data = np.linspace(-6,6,100)\n",
        "y_data = htan(x_data)\n",
        "dy_data = der_htan(x_data)\n",
        "# Graph\n",
        "plt.plot(x_data, y_data, x_data, dy_data)\n",
        "plt.title('htan Activation Function & Derivative')\n",
        "plt.legend(['htan','der_htan'])\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "nr4NE3BirZlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The ReLU (Rectified Linear Unit) activation function"
      ],
      "metadata": {
        "id": "gybDVoLvBm1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU (Rectified Linear Unit) activation function has several advantages and disadvantages in deep learning:\n",
        "\n",
        "### Advantages of ReLU Activation Function:\n",
        "- **Nonlinearity Introduction:** ReLU introduces nonlinearity to neural networks, aiding in learning complex relationships in data[1].\n",
        "- **Vanishing Gradient Mitigation:** Helps mitigate the vanishing gradient problem during model training, enabling better convergence of gradient descent[2].\n",
        "- **Computational Efficiency:** More computationally efficient than functions like Sigmoid, as it only requires a max() function without expensive exponential operations[3].\n",
        "- **Linear Behavior:** Easier optimization due to its linear or close to linear behavior, making it popular for deep learning models[4].\n",
        "\n",
        "### Disadvantages of ReLU Activation Function:\n",
        "- **Zeroing Negative Values:** All negative values immediately become zero, potentially affecting the model's ability to fit or train properly[1].\n",
        "- **Dying ReLU Problem:** Neurons can become \"dead\" during training if too many activations fall below zero, hindering learning; this can be mitigated by using Leaky ReLU[3].\n",
        "- **Not Zero-Centric:** Not zero-centric, giving zero for negative values, which can impact the network's robustness[4].\n",
        "\n",
        "ReLU's advantages include its simplicity, effectiveness in training deep models, and computational efficiency, while its main drawback lies in the zeroing of negative values, which can lead to the dying ReLU problem. Leaky ReLU is a variant that addresses some of these issues by allowing a small linear component for negative values.\n",
        "\n",
        "Citations:\n",
        "\n",
        "[1] https://builtin.com/machine-learning/relu-activation-function\n",
        "\n",
        "[2] https://www.datasciencecentral.com/deep-learning-advantages-of-relu-over-sigmoid-function-in-deep/\n",
        "\n",
        "[3] https://artemoppermann.com/activation-functions-in-deep-learning-sigmoid-tanh-relu/\n",
        "\n",
        "[4] https://www.linkedin.com/pulse/top-10-activation-functions-advantages-disadvantages-dash\n",
        "\n",
        "[5] https://vidyasheela.com/post/what-are-the-advantages-and-disadvantages-of-relu-activation-function"
      ],
      "metadata": {
        "id": "aqU8jVOUBoyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rectified Linear Unit (ReLU)\n",
        "def ReLU(x):\n",
        " data = [max(0,value) for value in x]\n",
        " return np.array(data, dtype=float)\n",
        "\n",
        "# Derivative for ReLU\n",
        "def der_ReLU(x):\n",
        " data = [1 if value>0 else 0 for value in x]\n",
        " return np.array(data, dtype=float)\n",
        "\n",
        "\n",
        "# Generative data to plot\n",
        "x_data = np.linspace(-10, 10, 100)\n",
        "y_data = ReLU(x_data)\n",
        "dy_data = der_ReLU(x_data)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(x_data, y_data, x_data, dy_data)\n",
        "plt.title('ReLU Activation Function & Derivative')\n",
        "plt.legend(['ReLU','der_ReLU'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_F0C50ZYCDwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}